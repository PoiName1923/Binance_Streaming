# Service
services:
  # Jupyter
  jupyter:
    build:
      context: ./docker
      dockerfile: dockerfile.jupyter
    container_name: jupyter
    restart: unless-stopped
    ports:
      - "8888:8888"
    volumes:
      - ./test:/home/jovyan/work
      - ./jars:/home/jovyan/work/jars
    environment:
      - JUPYTER_ENABLE_LAB=yes
      - JUPYTER_TOKEN=ndtien
    networks:
      - project-networks
    command: start-notebook.sh
    healthcheck:
      test: [ "CMD", "curl", "--silent", "--fail", "http://localhost:8888" ]
      interval: 30s
      retries: 3
      timeout: 5s
      start_period: 10s
  #Kafka & Kafka UI:
  kafka:
    image: bitnami/kafka:4.0.0
    container_name: kafka
    restart: unless-stopped
    ports:
      - 9092:9092
    environment:
      - KAFKA_CFG_NODE_ID=0
      - KAFKA_CFG_PROCESS_ROLES=controller,broker
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=0@kafka:9093
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE=true
    networks:
      - project-networks
    healthcheck:
      test: [ "CMD", "curl", "--silent", "--fail", "http://localhost:8081" ]
      interval: 30s
      retries: 3
      timeout: 5s
      start_period: 10s
  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    restart: unless-stopped
    ports:
      - "8081:8080"
    depends_on:
      - kafka
    environment:
      - KAFKA_CLUSTERS_0_NAME=kafka
      - KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS=kafka:9092
    networks:
      - project-networks
  # Spark
  spark-master:
    image: bitnami/spark:3.5.0
    container_name: spark-master
    hostname: spark-master
    restart: unless-stopped
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_HOST=spark-master
      - SPARK_MASTER_PORT=7077
      - SPARK_DAEMON_MEMORY=1G
    ports:
      - "7077:7077"
      - "8080:8080"
    networks:
      - project-networks
  spark-worker-1:
    image: bitnami/spark:3.5.0
    container_name: spark-worker-1
    hostname: spark-worker-1
    restart: unless-stopped
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=4G
      - SPARK_WORKER_CORES=4
    networks:
      - project-networks
  spark-worker-2:
    image: bitnami/spark:3.5.0
    container_name: spark-worker-2
    hostname: spark-worker-2
    restart: unless-stopped
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=4G
      - SPARK_WORKER_CORES=4
    networks:
      - project-networks
  # MiniO (Giả lập S3)
  minio:
    image: minio/minio
    container_name: minio
    hostname: minio
    restart: unless-stopped
    depends_on:
      - spark-master
    ports:
      - "9000:9000" # API S3
      - "9001:9001" # Web UI
    environment:
      MINIO_ROOT_USER: ndtien2004
      MINIO_ROOT_PASSWORD: ndtien2004
    command: server /data --console-address ":9001"
    volumes:
      - E:/minio_data:/data
    networks:
      - project-networks
  # Data Process
  streaming_data:
    build:
      context: ./docker
      dockerfile: dockerfile.python
    container_name: streaming_data
    restart: unless-stopped
    volumes:
      - ./data_process:/app
    command: [ "sh", "-c", "python kafka_product.py" ]
    networks:
      - project-networks
  streaming_transform:
    build:
      context: ./docker
      dockerfile: dockerfile.python
    container_name: streaming_transform
    depends_on:
      - streaming_data
    restart: unless-stopped
    volumes:
      - ./data_process:/app
      - ./jars:/app/jars
    command: [ "sh", "-c", "python trans_process.py" ]
    networks:
      - project-networks
  # Airflow
  postgres:
    image: postgres:16
    restart: unless-stopped
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: airflow
    volumes:
      - ./data/postgres-db-volume:/var/lib/postgresql/data
    networks:
      - project-networks
  airflow-webserver:
    build:
      context: ./docker
      dockerfile: dockerfile.airflow
    container_name: airflow-webserver
    depends_on:
      - postgres
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres/airflow
      AIRFLOW__WEBSERVER__SECRET_KEY: ${AIRFLOW_SECRET_KEY}
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      PYTHONPATH: /opt/airflow
      TZ: Asia/Ho_Chi_Minh
    volumes:
      - ./dags:/opt/airflow/dags
      - ./data_process:/opt/airflow/dags/code_python
      - ./jars:/opt/airflow/jars
    ports:
      - 8083:8080
    command: >
      bash -c "
        airflow db init &&
        airflow users create --username ${AIRFLOW_USER} --password ${AIRFLOW_PASSWORD} --firstname ${AIRFLOW_FIRSTNAME} --lastname ${AIRFLOW_LASTNAME} --role Admin --email ${AIRFLOW_EMAIL} &&
        airflow webserver
      "
    restart: unless-stopped
    networks:
      - project-networks
  airflow-scheduler:
    build:
      context: ./docker
      dockerfile: dockerfile.airflow
    container_name: airflow-scheduler
    depends_on:
      - airflow-webserver
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres/airflow
      AIRFLOW__WEBSERVER__SECRET_KEY: ${AIRFLOW_SECRET_KEY}
      TZ: Asia/Ho_Chi_Minh
      PYTHONPATH: /opt/airflow
    volumes:
      - ./dags:/opt/airflow/dags
      - ./data_process:/opt/airflow/dags/code_python
      - ./jars:/opt/airflow/jars
    command: airflow scheduler
    restart: unless-stopped
    networks:
      - project-networks
# Networks
networks:
  project-networks:
volumes:
  data:


